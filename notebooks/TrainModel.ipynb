{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41b4e3da",
   "metadata": {},
   "source": [
    "# Resume Generation Model using TensorFlow\n",
    "\n",
    "This code builds and trains a **sequence-to-sequence model** for **resume generation** using TensorFlow and Keras.\n",
    "\n",
    "### Key Steps:\n",
    "1. **Import Libraries**: Uses NumPy, TensorFlow, and Scikit-learn.\n",
    "2. **Define Model (`build_resume_model`)**:\n",
    "   - An **encoder-decoder architecture** with:\n",
    "     - `Dense` layer for encoding input features.\n",
    "     - `LSTM` layer for sequence generation.\n",
    "     - `Softmax` output for token prediction.\n",
    "3. **Generate Sample Data**:\n",
    "   - `X`: 100 samples with 10 features.\n",
    "   - `y`: 100 samples with 10 timesteps and 2 output classes.\n",
    "4. **Train & Evaluate**:\n",
    "   - Model is trained for **5 epochs** and evaluated on test data.\n",
    "\n",
    "**Output**: Training loss, accuracy, and evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a63b1b9c-34a4-4e0c-8172-1a2276d18b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.3533 - loss: 0.7125\n",
      "Epoch 2/5\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7180 - loss: 0.7128\n",
      "Epoch 3/5\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.3264 - loss: 0.7011\n",
      "Epoch 4/5\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.2678 - loss: 0.6992\n",
      "Epoch 5/5\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4052 - loss: 0.6928\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301ms/step - accuracy: 0.4400 - loss: 0.6492\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6491801738739014, 0.4399999678134918]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define a simple sequence-to-sequence model for resume generation\n",
    "def build_resume_model(input_dim, output_dim, latent_dim=256, timesteps=10):\n",
    "    \"\"\"\n",
    "    Build a simple sequence-to-sequence model for resume generation.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    input_dim : int\n",
    "        The size of the input features (number of input features for job title, skills, etc.).\n",
    "    output_dim : int\n",
    "        The size of the output (resume tokens).\n",
    "    latent_dim : int, default=256\n",
    "        The size of the latent space for the encoding/decoding process.\n",
    "    timesteps : int, default=10\n",
    "        Number of timesteps for sequential processing in the decoder.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    model : keras.Model\n",
    "        The trained model for resume generation.\n",
    "    \"\"\"\n",
    "    # Input layer for resume features (job title, skills, experience)\n",
    "    encoder_inputs = layers.Input(shape=(input_dim,))\n",
    "    \n",
    "    # Latent space for the encoding process (transform input to dense representation)\n",
    "    encoder = layers.Dense(latent_dim, activation='relu')(encoder_inputs)\n",
    "    \n",
    "    # Reshape encoder output to match LSTM expected input (batch_size, timesteps, features)\n",
    "    encoder_reshaped = layers.RepeatVector(timesteps)(encoder)\n",
    "\n",
    "    # Decoder layers for resume generation (text generation)\n",
    "    decoder_lstm = layers.LSTM(latent_dim, return_sequences=True)(encoder_reshaped)\n",
    "    \n",
    "    # Output layer to generate token probabilities for each step\n",
    "    decoder_output = layers.Dense(output_dim, activation='softmax')(decoder_lstm)\n",
    "    \n",
    "    # Build the model with the encoder and decoder\n",
    "    model = tf.keras.Model(inputs=encoder_inputs, outputs=decoder_output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Generate sample input data (resume features)\n",
    "X = np.random.rand(100, 10)  # 100 samples, 10 features (job title, skills, etc.)\n",
    "y = np.random.randint(0, 2, (100, 10, 2))  # 100 samples, 10 timesteps, 2 output classes (resume tokens)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Build and train the model\n",
    "model = build_resume_model(input_dim=10, output_dim=2, timesteps=10)  # Adjust dimensions as needed\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=16)\n",
    "\n",
    "# Evaluate model performance \n",
    "model.evaluate(X_test, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmse802",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
